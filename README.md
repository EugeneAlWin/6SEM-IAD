# `Конспект лабораторных`

## Перед началом работы

(vscode)  
`python -m venv ./.venv`  
`./.venv/Scripts/activate`  
`pip3 install -r requirements.txt`

## Лабораторная работа № 1

### `Цель: построение модели машинного обучения, которая сможет обучиться на основе характеристик ирисов, уже классифицированных по сортам, и предскажет сорт для нового цветка ириса`

Поскольку у нас есть примеры, то задача является задачей обучения с учителем.
| Термин | Значение |
| --------------------------- | --------------------------------------------------------------------------- |
| Задача классификации | Прогнозирование (один из сортов ириса) |
| Классы | Возможные ответы |
| Метка | Сорт, к которому принадлежит цветок (конкретная точка данных) |
| Загружаемые данные | |
| Примеры | В машинном обучении - отдельные элементы |
| Характеристики или признаки | Их свойства |
| Форма массива данных | Определяется количеством примеров |
| Метрики эффективности: обучающий и тестовый наборы ||
| Обучающие данные (набор) | Часть данных, которые используются для построения модели машинного обучения |
| Тестовые данные | Часть данных, которые используются для оценки качества модели |
| Просмотр данных ||
| Диаграмма рассеяния | один из способов исследования данных (визуализация)|
| Матрица диаграмм рассеяния | размещение набора данных с 3+ признаками |

---

## Лабораторная работа № 2

Алгоритм К ближайших соседей (суть):  
Создается цветок с параметрами. Сравнивается с готовым набором данных (из лаб. 1) и выдается предположение о сорте цветка. Потом идет проверка прогноза на точность.
K означает количество ближайших соседей. Правильность -процент цветов, для которых модель предсказала правильный сорт.  
`fit()` - построение модели на обучающем наборе данных  
`predict()` - применение модели к новым данным (прогноз)  
`score()` - оценка точности модели

---

## Лабораторная работа № 3

### Классификация и регрессия

Две основные задачи машинного обучения: `классификация` и `регрессия`.

`Цель классификации` - спрогнозирвать метку класса, которая представляет собой выбор из заранее определенного списка возможных вариантов.

Классификация делится на: бинарную и мультиклассовую.

`Бинарная классификация` - частный случай разделения на два класса. Можно представить как попытку ответить на поставленный вопрос в формате "да/нет".

`Цель регрессии` - спрогнозивать непрерывное число или число с плавающей точкой (вещественное). Прогнозируемое значение называется `суммой` и может быть любым числом в заданном диапазоне.

Простой способ отличить - спросить, заложена ли в полученном ответе определенная непрерывность. Результаты непрерывно связаны - задача регрессии.

Модель обладает способностью `обобщать`, если может выдавать точные прогнозы на ранее не встечавшихся данных.

`Переобучение` - построение модели, которая слишком сложна для имеющегося объема информации. Происходит, когда модель слишком точно подстраивается под особенности обучающего набора и на выходе получается модель, которая хорошо работает на обучающем наборе, но не умеет обобщать результат на новые данные.

`Недообучение` - слишком простая модель, обратная "переобученной".

`Оптимальная точка` - наилучшая обобщающая способность.

Сложность модели связана с изменчивостью входных данных. Больше разнообразие точек в наборе - более сложная модель может использоваться. **Дублирования** лучше избегать.

```python
wave #синтетический набор для иллюстрации алгоритмов регрессии
```

Набор данных имеет единственный входной признак и непрерывную целевую переменную или `отклик`, который мы хотим смоделировать.

`Низкоразмерный набор (low-dimensional)` - набор с небольшим числом признаков.
Вывод может не подтвердится для `высокоразмерного набора (high-dimensional)`.

`Конструирование признаков` - включение производных признаков.

```python
load_extended_boston # набор данных с производными признаками
```

### Алгоритм k ближайших соседей

`Голосование` - для каждой точки тестового набора мы подсчитываем кол-во соседей, относящихся к классу 0, и кол-во соседей, относящихся к классу 1. Затем мы присваиваем точке тестового набора наиболее часто встречающийся класс. Используется при рассмотрении более одного соседа.

`Граница принятия решений` - граница, которая разбивает плоскость на две области: где алгоритм присваивает класс 0, и где класс 1.

Алгоритм регрессии k ближайших соседей реализован в классе `KNeighborsRegressor`

Метод `score()` возвращает значение R<sup>2</sup> (коэффициент детерминации), позволяет оценить качество модели.

В классификаторе `KNeighbors` два важных параметра: кол-во соседей и мера расстояния между точками данных.

Преимущества метода ближайших соседей:

- модель легко интерпретировать
- приемлемое качество без необходимости использования большого кол-ва настроек.

Недостатки:

- Занимает время, когда набор данных очень большой.
- Важно проводить предварительную обработку данных.
- Не хорош, когда признаков 100+ и особенно плох, когда много признаков в большей части наблюдения имеют 0-е значения (`разреженные наборы данных`).

---

## Лабораторная работа № 4

### Линейные модели. Метод наименьших квадратов.

Линейные модели дают прогноз, используя линейную функцию.

### Линейная регрессия (обычный метод наименьших квадратов)

Линейная регрессия или обычный метод наименьших квадратов - самый простой метод регрессии.

`Линейная регрессия` находит параметры w и b, которые минимизируют ошибку (mean squared error) между спрогнозированными и фактическими ответами в обучающем наборе.

`Среднеквадратичная ошибка` равна сумме квадратов разностей между спрогнозированными и
фактическими значениями. Линейная регрессия проста, что является преимуществом, но в то же
время у нее нет инструментов, позволяющих контролировать сложность модели.

Параметры «наклона» (w), также называемые `весами` или `коэффициентами` (coefficients),
хранятся в атрибуте `coef_`, тогда как `сдвиг (offset)` или `константа (intercept)`, обозначаемая как b,
хранится в атрибуте `intercept_`.

Атрибут `intercept_` - это всегда отдельное число с плавающей точкой, тогда как атрибут `coef_` - это массив NumPy, в котором каждому элементу соответствует входной признак.

### Гребневая регрессия

Гребневая регрессия также является линейной моделью регрессии, поэтому ее формула аналогична той, что используется в обычном методе наименьших квадратов.

Все элементы w должны быть близки к нулю.

`Регуляризация` означает явное ограничение модели для предотвращения переобучения.
Регуляризация, использующаяся в гребневой регрессии, известна как L2 регуляризация.
Гребневая регрессии реализована в классе `linear_model.Ridge`.

Модель `Ridge` позволяет найти компромисс между простотой модели (получением коэффициентов, близких к нулю) и качеством ее работы на обучающем наборе. Компромисс между простотой модели и качеством работы на обучающем наборе может быть задан пользователем при помощи параметра `alpha`. Увеличение alpha заставляет коэффициенты сжиматься до близких к нулю значений, что снижает качество работы модели на обучающем наборе, но может улучшить ее обобщающую способность.

`Кривые обучения` - это график, показывающий зависимость между оценкой модели и количеством используемых данных.

### Лассо

Является альтернативой `Ridge`.
Как и гребневая регрессия, лассо также сжимает коэффициенты до близких к нулю значений, но несколько иным способом, называемым `L1` регуляризацией. Результат `L1` регуляризации заключается в том, что при использовании лассо некоторые коэффициенты становятся равны
точно нулю.

---

## Лабораторная работа № 5

### Линейные модели для классификации.

Для линейных моделей классификации `граница принятия решений` (`decision boundary`) является линейной функцией аргумента. Другими словами, (бинарный) линейный классификатор – это классификатор, который разделяет два класса с помощью линии, плоскости или гиперплоскости.

Существует масса алгоритмов обучения линейных моделей. Два критерия задают различия между алгоритмами:

- Измеряемые метрики качества подгонки обучающих данных
- Факт использования регуляризации и вид регуляризации, если она используется

В силу технико-математических причин невозможно скорректировать `w` и `b`, чтобы
минимизировать количество неверно классифицированных случаев, выдаваемое алгоритмами,
как можно было бы надеяться. С точки зрения поставленных нами целей и различных сфер
применения различные варианты метрик качества подгонки (так называемые `функции потерь`
или `loss functions`) не имеют большого значения.

Двумя наиболее распространенными алгоритмами линейной классификации являются
`логистическая регрессия (logistic regression)`, реализованная в классе
`linear_model.LogisticRegression`, и `линейный метод опорных векторов (linear support vector machines)` или `линейный SVM`, реализованный в классе `svm.LinearSVC` (`SVC` расшифровывается как `support vector classifier` – классификатор опорных векторов).

Логистическая регрессия является алгоритмом классификации, а не алгоритмом регрессии.

По умолчанию обе модели используют `L2 регуляризацию`, тот же самый метод, который используется в гребневой регрессии.

Для `LogisticRegression` и `LinearSVC` компромиссный параметр, который определяет степень регуляризации, называется `C`, и более высокие значения `C` соответствуют меньшей регуляризации. Другими словами, когда вы используете высокое значение параметра `C`, `LogisticRegression` и `LinearSVC` пытаются подогнать модель к обучающим данным как можно лучше, тогда как при низких значениях параметра `C` модели делают больший акцент на поиске вектора коэффициентов (`w`), близкого к нулю.

## Лабораторная работа № 6

### Линейные модели для мультиклассовой классификации.

Многие линейные модели классификации предназначены лишь для бинарной классификации и не распространяются на случай
мультиклассовой классификации (за исключением логистической регрессии). Общераспространненный подход, позволяющий
распространить алгоритм бинарной классификации на случай мультиклассовой классификации называет подходом `один против остальных`
(`one-vs.-rest`).

В этом подходе `для каждого класса` строится `бинарная модель`, которая пытается `отделить` этот `класс` `от всех остальных`,
в результате чего количество моделей определяется количеством классов. Для получения прогноза точка тестового набора
подается на все бинарные классификаторы. Классификатор, который выдает по своему классу наибольшее значение, «побеждает»
и метка этого класса возвращается в качестве прогноза.

`Используя бинарный классификатор` для каждого класса, мы `получаем один вектор коэффициентов (w) и одну константу (b)
 по каждому классу`. Класс, который получает наибольшее значение согласно нижеприведенной формуле, становится
присвоенной меткой класса.

`Математический аппарат` мультиклассовой логистической регрессии несколько отличается от подхода «`один против остальных`»,
однако он также дает один вектор коэффициентов и константу для каждого класса и использует тот же самый способ получения
прогнозов.

Атрибут `coef_` имеет форму (3, 2) (матрица 3x2), это означает, что каждая строка `coef_`
содержит вектор коэффициентов для каждого из трех классов, а каждый столбец содержит
коэффициент для конкретного признака (в этом наборе данных их два). Атрибут `intercept_` теперь
является одномерным массивом, в котором записаны константы классов.

`Про середину треугольника`

> Однако что насчет треугольника в середине графика? Все три бинарных классификатора
> относят точки, расположенные там, к «остальным». Какой класс будет присвоен точке,
> расположенной в треугольнике? Ответ – класс, получивший наибольшее значение по формуле
> классификации, то есть класс ближайшей линии.

#### Преимущества, недостатки и параметры

Основной параметр линейных моделей – параметр регуляризации, называемый `alpha` в
моделях регрессии и `C` в `LinearSVC` и `LogisticRegression`. Большие значения `alpha` или маленькие
значения `C` означают простые модели. Конкретно для регрессионных моделей настройка этих
параметров имеет весьма важное значение. Как правило, поиск `C` и `alpha` осуществляется по
логарифмической шкале. Кроме того вы должны решить, какой вид регуляризации нужно
использовать: `L1` или `L2`. Если вы полагаете, что на самом деле важны лишь некоторые признаки,
следует использовать `L1`. В противном случае используйте установленную по умолчанию `L2`
регуляризацию. Еще `L1` регуляризация может быть полезна, если интерпретируемость модели
имеет важное значение.

Преимущества линейных моделей:

- Очень быстро обучаются.
- Быстро прогнозируют.
- Масштабируются на очень большие наборы данных, также хорошо работают с разреженными
  данными.
- Позволяют относительно легко понять, как был получен прогноз, при помощи формул, которые мы видели
  ранее для регрессии и классификации.

Как правило, линейные модели хорошо работают, когда количество признаков превышает
количество наблюдений. Кроме того, они часто используются на очень больших наборах данных,
просто потому, что не представляется возможным обучить другие модели.

Вместе с тем в низкоразмерном пространстве альтернативные модели могут показать более высокую обобщающую способность.

#### Цепочка методов (method chaining)

В общем, тут грится, что методы можно вызывать в одну строчку (через точку), так как они возвращают объект `self`.
Но надо аккуратно, потому что можно сделать код трудночитаемым. Не знаю зачем она про это написала, но пусть так.

## Лабораторная работа № 7

### Наивные байесовские классификаторы (Naive Bayesian)

Наивные `байесовские классификаторы` представляют собой семейство классификаторов,
которые очень схожи с `линейными моделями`. Однако они имеют тенденцию обучаться быстрее.
Цена, которую приходится платить за такую эффективность – немного более низкая обобщающая
способность моделей Байеса по сравнению с линейными классификаторами типа `LogisticRegression` и `LinearSVC`

Причина, по которой наивные байесовские модели столь эффективны, заключается в том,
что они оценивают параметры, рассматривая каждый признак отдельно и по каждому признаку
собирают простые статистики классов. В `scikit-learn` реализованы три вида наивных байесовских
классификаторов: `GaussianNB`, `BernoulliNB` и `MultinomialNB`. `GaussianNB` можно применить к
любым непрерывным данным, в то время как `BernoulliNB` принимает бинарные данные,
`MultinomialNB` принимает счетные или дискретные данные (то есть каждый признак представляет
собой подсчет целочисленных значений какой-то характеристики, например, речь может идти о
частоте встречаемости слова в предложении). `BernoulliNB` и `MultinomialNB` в основном
используются для классификации текстовых данных

Классификатор `BernoulliNB` подсчитывает ненулевые частоты признаков по каждому
классу.

```python
X = np.array([[0, 1, 0, 1], [1, 0, 1, 1], [0, 0, 0, 1], [1, 0, 1, 0]])
y = np.array([0, 1, 0, 1])
```

Здесь у нас есть четыре точки данных с четырьмя бинарными признаками. Есть два класса
0 и 1. Для класса 0 (первая и третья точки данных) первый признак равен нулю два раза и отличен
от нуля ноль раз, второй признак равен нулю один раз и отличен от нуля один раз и так далее.

Чё?

#### Преимущества, недостатки и параметры

Две другие наивные байесовские модели `MultinomialNB` и `GaussianNB`, немного отличаются
с точки зрения вычисляемых статистик. `MultinomialNB` принимает в расчет среднее значение
каждого признака для каждого класса, в то время как `GaussianNB` записывает среднее значение, а
также стандартное отклонение каждого признака для каждого класса.
Для получения прогноза точка данных сравнивается со статистиками для каждого класса и
прогнозируется наиболее подходящий класс.

`MultinomialNB` и `BernoulliNB` имеют один параметр `alpha`, который контролирует сложность
модели. Параметр `alpha` работает следующим образом: алгоритм добавляет к данным зависящее
от `alpha` определенное количество искусственных наблюдений с положительными значениями для
всех признаков. Это приводит к «сглаживанию» статистик. Большее значение `alpha` означает более
высокую степень сглаживания, что приводит к построению менее сложных моделей. Алгоритм
относительно устойчив к разным значениям `alpha`. Это означает, что значение `alpha` не оказывает
значительного влияния на хорошую работу модели.

`GaussianNB` в основном используется для данных с очень высокой размерностью, тогда как
остальные наивные байесовские модели широко используются для разреженных дискретных
данных, например, для текста. `MultinomialNB` обычно работает лучше, чем `BernoulliNB`, особенно
на наборах данных с относительно большим количеством признаков, имеющих ненулевые частоты.

---

## Лабораторная работа № 8

### Деревья решений.

Суть деревьев: если - то. Каждый узел дерева либо представляет собой либо вопрос, либо терминальный узел (его еще называют `листом` или `leaf`), который содержит ответ. Ребра соединяют вышестоящие узлы с нижестоящими.
Построение дерева решений означает построение последовательности правил «если... то...», которая приводит нас к истинному ответу максимально коротким путем. В машинном обучении эти правила называются `тестами` (`tests`).
Верхний узел - `корень` (`root`).
Лист дерева, который содержит точки данных, относящиеся к одному и тому же значению целевой переменной, называется `чистым` (`pure`).

### Контроль сложности деревьев

Две стратегии предотвращения обучения:

- ранняя остановка построения дерева, называемая `предварительной обрезкой `(`pre-pruning`).
- построение дерева с последующим удалением или сокращением малоинформативных узлов, называемое `пост-обрезкой`(`post-pruning`).

Возможные критерии предварительной обрезки включают в себя ограничение максимальной глубины дерева,ограничение максимального количества листьев или минимальное количество наблюдений в узле, необходимое для разбиения.В библиотеке scikit-learn деревья решений реализованы в классах `DecisionTreeRegressor` и `DecisionTreeClassifier`

`Важность признаков` (`feature importance`) - оценивает, насколько важен каждый признак сточки зрения получения решений. Это число варьируется в диапазоне от 0 до 1 для каждого признака, где 0 означает «не используется вообще», а 1 означает, что «отлично предсказывает целевую переменную».

### Преимущества, недостаткии параметры

Обычно, чтобы предотвратить переобучение, достаточно выбрать одну из стратегий предварительной обрезки – настроить `max_depth`, `max_leaf_nodes` или `min_samples_leaf`.

Плюсы:

- Легко понимается неспециалистами
- Не требуют масштабирования данных

Недостаток:

- Склонны к переобучению и имеют низкую обобщающую способность.

---

## Лабораторная работа № 9

### Ансамбли. Случайный лес. Градиентный бустинг деревьев регресии.

`Ансамбли` (`ensembles`) – это методы, которые сочетают в себе множество моделей машинного обучения, чтобы в итоге получить более мощную модель. Моделей много, но две доказали свою эффективность: `случайный лес деревьев решений` и `градиентный бустинг деревьев решений`.

Недостатком деревьев решений является склонность к переобучению. `Случайный лес` один из способов решения. `Случайный лес` – это набор деревьев решений, где каждое дерево немного отличается от остальных.

> Идея случайного леса заключается в том, что каждое дерево может довольно хорошо прогнозировать, но скорее всего переобучается на части данных. Если мы построим много деревьев, которые хорошо работают и переобучаются с разной степенью, мы можем уменьшить переобучение путем усреднения их результатов.

> Херня идея короч. © EV-GEN.

Существует `две техники`, позволяющие получить рандомизированные деревья в рамках случайного леса: сначала выбираем точки данных (наблюдения), которые будут использоваться для построения дерева, а затем отбираем признаки в каждом разбиении.

### Построение случайного леса

Для построения модели случайных лесов необходимо определиться с количеством
деревьев (параметр `n_estimators` для `RandomForestRegressor` или `RandomForestClassifier`).

Для построения дерева мы сначала
сформируем `бутстреп-выборку` (`bootstrap sample`) наших данных. То есть из `n_samples` примеров
мы случайным образом выбираем пример с возвращением `n_samples` раз.

`Количество отбираемых признаков` контролируется параметром `max_features`. Отбор подмножества признаков повторяется отдельно для каждого узла, поэтому в каждом узле дерева
может быть принято решение с использованием «своего» подмножества признаков.

`max_features` не ставить в `1` и `n_features`. При 1 не отбираются признаки, при max в отбор не будет привнесена случайность.

Чтобы дать прогноз для случайного леса, алгоритм сначала дает прогноз для каждого
дерева в лесе. Для регрессии мы можем усреднить эти результаты, чтобы получить наш
окончательный прогноз. Для классификации используется стратегия `«мягкого голосования»`. Это
означает, что каждый алгоритм дает `«мягкий»` прогноз, вычисляя вероятности для каждого класса.
Эти вероятности усредняются по всем деревьям и прогнозируется класс с наибольшей
вероятностью.

Деревья, которые строятся в рамках случайного леса, сохраняются в атрибуте `estimator_`.

### Преимущества и недостатки

- Обладают высокой прогнозной силой
- Часто дают хорошее качество модели без утомительной настройки параметров.
- Не требуют масштабирования данных.
- Неспециалисту легко воспринять.

---

- Плохо работает на данных очень высокой размерности, разреженных данных, например, на текстовых данных.
- Требует больше памяти и медленнее обучается и прогнозирует, чем линейные модели. Если время и память имеют важное значение, имеет смысл вместо случайного леса использовать линейную модель.

`Градиентный бустинг деревьев регрессии` – еще один ансамблевый метод, который
объединяет в себе множество деревьев для создания более мощной модели.

В отличие от `случайного леса`, `градиентный бустинг` строит последовательность деревьев, в которой каждое дерево пытается исправить ошибки предыдущего.

Идея в объединении множества простых
моделей (в данном контексте известных под названием `слабые ученики` или `weak learners`), деревьев небольшой глубины

`learning_rate` контролирует, насколько сильно каждое дерево будет пытаться исправить ошибки предыдущих деревьев. Более высокая скорость обучения означает, что каждое дерево может внести более сильные корректировки и это позволяет
получить более сложную модель.

### Преимущества и недостатки

Недостаток заключается в том, что они требуют тщательной настройки параметров и для обучения может потребоваться много времени
Как и остальные модели на
основе дерева, он также плохо работает на высокоразмерных разреженных данных и хорошо на данных, представляющих смесь
бинарных и непрерывных признаков.

Основные параметры градиентного бустинга деревьев:

- количество деревьев (`n_estimators`)
- скорость обучения (`learning_rate`),
- `max_depth` (или, как альтернатива,
  `max_leaf_nodes`), направленный на уменьшение сложности каждого дерева.

> Ну и параша.

## Лабораторная работа № 10

### Метод опорных векторов

`Ядерный метод опорных векторов` (часто его просто называют `SVM`) – это расширение метода опорных векторов, оно позволяет получать более сложные модели, которые не сводятся к построению простых гиперплоскостей в пространстве.

### Ядерный трюк (kernel trick)

`Ядерный трюк` (`kernel trick`) - математический трюк, который позволяет нам обучить классификатор в многомерном пространстве, фактически не прибегая к вычислению нового, возможно, очень высокоразмерного пространства. Он непосредственно вычисляет евклидовы расстояния (более точно, скалярные произведения точек данных), чтобы получить расширенное пространство признаков без фактического добавления новых признаков.

Существуют два способа поместить данные в `высокоразмерное пространство`, которые чаще всего используются методом опорных векторов: `полиномиальное ядро`, которое вычисляет все возможные полиномиальные комбинации исходных признаков до определенной степени, и `ядро RBF` (радиальная базисная функция), также известное как гауссовское ядро.

`Опорные вектора` - точки, которые лежат на границе между классами. Нужны для определения границы принятия решений.

Расстояние между точками данных измеряется с помощью `гауссовского ядра`:
k<sub>rbf</sub>(x<sub>1</sub>, x<sub>2</sub>)=exp(x<sub>1</sub>-x<sub>2</sub>)<sup>2</sup> (вроде)

### Параметры

Параметр `gamma` - это параметр формулы, приведенной в предыдущем разделе. Он регулирует ширину гауссовского ядра. Параметр `gamma` задает степень близости расположения точек. Параметр `С` представляет собой параметр регуляризации, аналогичный тому, что использовался в линейных моделях. Он ограничивает важность каждой точки (точнее, ее `dual_coef_`).
Низкое значение `gamma` означает медленное изменение решающей границы, которое дает модель низкой сложности, в то время как высокое значение `gamma` дает более сложную модель.

Общераспространенный `метод масштабирования` для ядерного SVM заключается в масштабировании данных так, чтобы все признаки принимали значения от 0 до 1.

### Преимущества и недостатки

Преимущества:

- Мощная прогнозная сила
- Различные наборы данных

Недостатки:

- Плохо масштабируются.
- Тщательная настройка параметров

---

## Лабораторная работа № 11

### Нейронные сети

`Многослойные персептроны` (`MLP`) также называют `простыми` (`vanilla`) нейронными сетями прямого распространения, а иногда и просто нейронными сетями.

`MLP` можно рассматривать как обобщение линейных моделей, которое прежде чем прийти к решению выполняет несколько этапов обработки данных.

В `MLP` процесс вычисления взвешенных сумм повторяется несколько раз. Сначала вычисляются `скрытые элементы` (`hidden units`), которые представляют собой промежуточный этап обработки. Они вновь объединяются с помощью взвешенных сумм для получения конечного результата.

Входной слой просто передает входы скрытому слою сети, либо без преобразования, либо выполнив сначала стандартизацию входов. Затем происходит вычисление взвешенной суммы входов для каждого элемента скрытого слоя, к ней применяется `функция активации` – обычно используются `нелинейные функции выпрямленный линейный элемент` (`rectified linear unit` или `relu`) или `гиперболический тангенс` (`hyperbolic tangent` или `tanh`).

`Relu` отсекает значения ниже нуля, в то время как `tanh` принимает значения от –1 до 1.

Существуют различные способы регулировать сложность нейронной сети: `количество  скрытых  слоев`, `количество  элементов  в  каждом  скрытом  слое `и `регуляризация` (`alpha`).

Важным свойством нейронных сетей является то, что их веса задаются случайным образом
перед началом обучения и случайная инициализация влияет на процесс обучения модели.

`MLP` демонстрирует довольно неплохую правильность, однако не столь хорошую, если
сравнивать с другими моделями. Как и в предыдущем примере с `SVC`, это, вероятно, обусловлено
масштабом данных. Нейронные сети также требуют того, чтобы все входные признаки были
измерены в одном и том же масштабе, в идеале они должны иметь среднее 0 и дисперсию 1.

### Преимущества и недостатки

Преимущества:

- способны обрабатывать информацию, содержащуюся в больших объемах данных
- строят невероятно сложные модели

Недостатки:

- Длительное время обучения
- Тщательная обработка данных

> Дальше идет что-то про оценку сложности в сетях, скрытые слои, миллионы весов и кококо. Мне лень писать про это, уж простите.

## Лабораторная работа № 12

### Оценка неопределенностей для классификаторов

В scikit-learn существует две различные функции, с помощью которых можно оценить
неопределенность прогнозов: `decision_function` и `predict_proba`. Возвращаемое значение представляет собой число с плавающей точкой для каждого
примера.Значение показывает, насколько сильно модель уверена в том, что точка данных
принадлежит «положительному» классу, в данном случае, классу 1. Положительное значение
указывает на предпочтение в пользу позиционного класса, а отрицательное значение – на
предпочтение в пользу «отрицательного» (другого) класса. Для бинарной классификации «отрицательный» класс – это всегда первый элемент атрибута
`classes_`, а «положительный» класс – второй элемент атрибута `classes_`. Таким образом, если вы
хотите полностью просмотреть вывод метода predict, вам нужно воспользоваться атрибутом classes\_.

Вывод метода `predict_proba` – это вероятность каждого класса и часто его легче понять, чем
вывод метода `decision_function`.

Первый элемент строки – это оценка вероятности первого класса, а второй элемент строки
– это оценка вероятности второго класса. Поскольку речь идет о вероятности, то значения в выводе
`predict_proba` всегда находятся в диапазоне между 0 и 1, а сумма значений для обоих классов
всегда равна 1.
Модель называется
`калиброванной` (`calibrated`), если вычисленная неопределенность соответствует
фактической: в калиброванной модели прогноз, полученный с 70%-ной определенностью, будет
правильным в 70% случаев.

В мультиклассовой классификации decision_function имеет форму (n_samples, n_classes) и
каждый столбец показывает «оценку определенности» для каждого класса, где высокая оценка
означает большую вероятность данного класса, а низкая оценка означает меньшую вероятность
этого класса.

Вывод predict_proba имеет точно такую же форму (n_samples, n_classes). И снова
вероятности возможных классов для каждой точки данных дают в сумме 1.

#### Ближайшие соседи

Подходит для небольших наборов данных, хорош в качестве базовой модели, прост в объяснении.

#### Линейные модели

Считается первым алгоритмом, который нужно попробовать, хорош для очень больших наборов данных, подходит для данных с очень высокой размерностью.

#### Наивный байесовский классификатор

Подходит только для классификации. Работает даже быстрее, чем линейные модели, хорош для очень больших наборов данных и высокоразмерных данных. Часто менее точен, чем линейные модели.

#### Деревья решений

Очень быстрый метод, не нужно масштабировать данные, результаты можно визуализировать и легко объяснить.

#### Случайные леса

Почти всегда работают лучше, чем одно дерево решений, очень устойчивый и мощный метод. Не нужно масштабировать данные. Плохо работает с данными очень высокой размерности и разреженными данными.

#### Градиентный бустинг деревьев решений

Как правило, немного более точен, чем случайный лес. В отличие от случайного леса медленнее обучается, но быстрее предсказывает и требует меньше памяти. По сравнению со случайным лесом требует настройки большего числа параметров.

#### Машины опорных векторов

Мощный метод для работы с наборами данных среднего размера и признаками,
измеренными в едином масштабе. Требует масштабирования данных, чувствителен к изменению
параметров.

#### Нейронные сети

Можно построить очень сложные модели, особенно для больших наборов данных.
Чувствительны к масштабированию данных и выбору параметров. Большим моделям требуется
много времени для обучения.

## Лабораторная работа № 13

### Обучение без учителя

Машинное обучение без учителя включает в себя все виды машинного обучения, когда ответ неизвестен и отсутствует учитель, указывающий ответ алгоритму.
Здесь два вида МО бех учителя:

- Преобразования данных
- Кластеризация

`Неконтролируемые  преобразования` (`unsupervised transformations`) – это алгоритмы, создающие новое представление данных, которое в отличие от исходного представления человеку или алгоритму машинного обучения будет обработать легче. Мы берем высокоразмерное представление данных, состоящее из
множества признаков, и находим новый способ представления этих данных, обобщая
основные характеристики и получая меньшее количество признаков.
Общераспространенное применение сокращения размерности – получение двумерного
пространства в целях визуализации

Еще одно применение неконтролируемых преобразований – поиск компонент, из
которых «состоят» данные.

`Алгоритмы кластеризации` (`clustering algorithms`) разбивают данные на отдельные группы схожих между собой элементов.

Главная проблема машинного обучения без учителя – `оценка полезности`
информации, извлеченной алгоритмом.

#### Различные виды предварительной обработки

- `StandardScaler` ( гарантирует, что для каждого признака среднее будет равно 0, а дисперсия будет равна 1, в
  результате чего все признаки будут иметь один и тот же масштаб.)
- `RobustScaler` (аналогичен, но вместо среднего и дисперсии использует
  медиану и квартили. Это позволяет `RobustScaler` игнорировать точки данных, которые сильно
  отличаются от остальных (например, ошибки измерений). Эти странные точки данных еще
  называются `выбросами` (`outliers`))
- `MinMaxScaler` (сдвигает данные таким образом, что все признаки
  находились строго в диапазоне от 0 до 1)
- `Normalizer` (Он масштабирует каждую точку данных таким образом, чтобы вектор признаков имел евклидову
  длину 1. Другими словами, он проецирует точку данных на окружность с радиусом 1 (или сферу в
  случае большого числа измерений). )

Метод `transform` используется в scikit-learn, когда модель возвращает новое представление данных.

## Лабораторная работа № 14

### Анализ главных компонент (PСA)

Анализ главных компонент представляет собой метод, который осуществляет вращение данных с тем, чтобы преобразованные признаки не коррелировали между собой.

Направления, найденные с помощью этого алгоритма, называются `главными компонентами` (`principal components`), поскольку они являются основными направлениями дисперсии данных.

Одним из наиболее распространенных применений PCA является визуализация высокоразмерных наборов данных.

`PCA выбеливание` (`whitening`) - преобразует компоненты к одному и тому же масштабу.

## Лабораторная работа № 15

### Факторизация неотрицательных матриц

Факторизация неотрицательных матриц – еще один алгоритм машинного обучения без учителя, цель которого – выделить полезные характеристики.
Процесс разложения данных на неотрицательную взвешенную сумму особенно полезен для данных, созданных в результате объединения (или наложения) нескольких независимых источников, например, аудиотреков с голосами нескольких людей, музыки с большим количеством инструментов.

В отличие от PCA, чтобы применить NMF к данным, мы должны убедиться, что они имеют положительные значения. Это означает, что для NMF расположение данных относительно начала координат (0, 0) имеет реальное значение.

Существует класс алгоритмов визуализации, называемых `алгоритмами множественного обучения` (`manifold learning algorithms`), которые используют гораздо более сложные графические представления данных и позволяют получить визуализации лучшего качества. Особенно полезным является алгоритм `t-SNE`.

Алгоритмы множественного обучения в основном направлены на визуализацию и поэтому редко используются для получения более двух новых характеристик. Некоторые из них, в том числе `t-SNE`, создают новое представление обучающих данных, но при этом не осуществляют преобразования новых данных.

Идея, лежащая в основе алгоритма `t-SNE`, заключается в том, чтобы найти двумерное представление данных, сохраняющее расстояния между точками наилучшим образом.

## Лабораторная работа № 16

### Кластеризация

`Кластеризация` (`clustering`) является задачей разбиения набора данных на группы, называемые кластерами. Цель – разделить данные таким образом, чтобы точки, находящие в одном и том же кластере, были очень схожи друг с другом, а точки, находящиеся в разных кластерах, отличались друг от друга.

#### Кластеризация k-средних

Сначала выбирается число кластеров k. После выбора значения k алгоритм k-средних отбирает точки, которые будут представлять `центры кластеров` (`cluster centers`). Затем для каждой точки данных вычисляется его евклидово расстояние до каждого центра кластера. Каждая точка назначается ближайшему центру кластера. Алгоритм вычисляет `центроиды` (`centroids`) – центры тяжести кластеров. Каждый центроид–это вектор, элементы которого представляют собой средние значения характеристик, вычисленные по всем точкам кластера. Центр кластера смещается в его центроид. Точки заново назначаются ближайшему центру кластера. Этапы изменения центров кластеров и переназначения точек итеративно повторяются до тех пор, пока границы кластеров и расположение центроидов не перестанут изменяться, т.е. на каждой итерации в каждый кластер будут попадать одни и те же точки данных.

#### Недостатки алгоритмаk-средних

Даже если вы знаете «правильное» количество кластеров для конкретного набора данных, алгоритм k-средних не всегда может выделить их. Каждый кластер определяется исключительно его центром, это означает, что каждый кластер имеет выпуклую форму. В результате этого алгоритм k-средних может описать относительно простые формы. Кроме того, алгоритм k-средних предполагает, что все кластеры в определенном смысле имеют одинаковый «диаметр», он всегда проводит границу между кластерами так, чтобы она проходила точно посередине между центрами кластеров.
Кроме того, алгоритм k-средних предполагает, что все направления одинаково важны для каждого кластера.
Алгоритм k-средних плохо работает, когда кластеры имеют более сложную форму.
